{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9193ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f7f7e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Take picture from webcam and save it ##########\n",
    "# For Camera Enable\n",
    "\n",
    "#import time\n",
    "# import cv2\n",
    "# video_port = 0\n",
    "# video = cv2.VideoCapture (video_port)\n",
    "# while True:\n",
    "#     ret , video_data = video.read()\n",
    "#     cv2.imshow(\"video_live\",video_data)\n",
    "#     if cv2.waitKey(10) == ord('a'):\n",
    "#         break\n",
    "# video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77048c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Go to this location for importing cv2 face data \"C:/ProgramData/anaconda3/Lib/site-packages/cv2/data\"\n",
    "\n",
    "face_cap = cv2.CascadeClassifier(\"C:/ProgramData/anaconda3/Lib/site-packages/cv2/data/haarcascade_frontalface_default.xml\")\n",
    "video_port = 0\n",
    "video_cap = cv2.VideoCapture (video_port)\n",
    "while True:\n",
    "    ret , video_data = video_cap.read()\n",
    "    col = cv2.cvtColor(video_data,cv2.COLOR_BGR2GRAY) # for making face color to black n white for getting face muscle properly. then again make color.\n",
    "    faces= face_cap.detectMultiScale(col, scaleFactor= 1.1, minNeighbors= 5, minSize=(30,30), flags = cv2.CASCADE_SCALE_IMAGE )\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(video_data,(x,y),(x+w, y+h), (0,255,0), 2)\n",
    "    cv2.imshow(\"video_live\",video_data)\n",
    "    if cv2.waitKey(10) == ord('a'):\n",
    "        break\n",
    "video_cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f78a33",
   "metadata": {},
   "source": [
    "# Face recognition using Deep Learning(CNN) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33da6dc",
   "metadata": {},
   "source": [
    "1. Convolutional neural networks (CNN) is used to convert digital image content into a single vector of numbers(numeric vector) representing the unique characteristics of the image.\n",
    "\n",
    "2. The column of numbers is inputted to a Dense fully connected Neural network layer against the labels, which image is cat, which image is bird etc.\n",
    "\n",
    "3. The classification model learns these numeric vector inputs against the labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3429b7b",
   "metadata": {},
   "source": [
    "### Reading the Images data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24318973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Found 244 images belonging to 16 classes.\n",
      "Found 244 images belonging to 16 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'face1': 0,\n",
       " 'face10': 1,\n",
       " 'face11': 2,\n",
       " 'face12': 3,\n",
       " 'face13': 4,\n",
       " 'face14': 5,\n",
       " 'face15': 6,\n",
       " 'face16': 7,\n",
       " 'face2': 8,\n",
       " 'face3': 9,\n",
       " 'face4': 10,\n",
       " 'face5': 11,\n",
       " 'face6': 12,\n",
       " 'face7': 13,\n",
       " 'face8': 14,\n",
       " 'face9': 15}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Deep Learning CNN model to recognize face\n",
    "'''This script uses a database of images and creates CNN model on top of it to test\n",
    "if the given image is recognized correctly or not'''\n",
    "\n",
    "'''######### IMAGE PRE-PROCESSING for TRAINING and TESTING data ########'''\n",
    "\n",
    "TrainingImagePath='D:/Coding Course/IVY Pro School/ML/Face Images/Final Training Images'\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#Understand more about ImageDataGenerator at below link\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-di\n",
    "\n",
    "# Defining pre-processing transformations on raw images of training data \n",
    "train_datagen = ImageDataGenerator (\n",
    "        rescale=1./255,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True)\n",
    "# Defining pre-processing transformations on raw images of testing data\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#Generating the Training Data\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "\n",
    "#Generating the Testing Data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    TrainingImagePath,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Printing class labels for each face\n",
    "test_set.class_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7539402",
   "metadata": {},
   "source": [
    "### Creating a list of faces and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d269382e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Face and its ID {0: 'face1', 1: 'face10', 2: 'face11', 3: 'face12', 4: 'face13', 5: 'face14', 6: 'face15', 7: 'face16', 8: 'face2', 9: 'face3', 10: 'face4', 11: 'face5', 12: 'face6', 13: 'face7', 14: 'face8', 15: 'face9'}\n",
      "\n",
      " The Number of output neurons: 16\n"
     ]
    }
   ],
   "source": [
    "'''###### Creating lookup table for all faces ####'''\n",
    "# class_indices have the numeric tag for each face \n",
    "TrainClasses = training_set.class_indices\n",
    "\n",
    "#Storing the face and the numeric tag for future reference\n",
    "ResultMap={}\n",
    "for faceValue, faceName in zip (TrainClasses.values(), TrainClasses.keys()): \n",
    "    ResultMap[faceValue]=faceName\n",
    "\n",
    "    # Saving the face map for future reference\n",
    "import pickle\n",
    "with open(\"ResultsMap.pkl\", 'wb') as f:\n",
    "    pickle.dump (ResultMap, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Mapping of Face and its ID\", ResultMap)\n",
    "\n",
    "# The number of neurons for the output layer is equal to the number of faces OutputNeurons-len (ResultMap)\n",
    "OutputNeurons=len(ResultMap)\n",
    "print('\\n The Number of output neurons:', OutputNeurons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c821a7d6",
   "metadata": {},
   "source": [
    "### Creating the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acde58e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "########## Create CNN deep learning model #####\n",
    "####*\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    " \n",
    "'''Initializing the Convolutional Neural Network'''\n",
    "classifier= Sequential()\n",
    " \n",
    "''' STEP--1 Convolution\n",
    "# Adding the first layer of CNN\n",
    "# we are using the format (64,64,3) because we are using TensorFlow backend\n",
    "# It means 3 matrix of size (64X64) pixels representing Red, Green and Blue components of pixels\n",
    "'''\n",
    "classifier.add(Convolution2D(32, kernel_size=(5, 5), strides=(1, 1), input_shape=(64,64,3), activation='relu'))\n",
    " \n",
    "'''# STEP--2 MAX Pooling'''\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    " \n",
    "'''############## ADDITIONAL LAYER of CONVOLUTION for better accuracy #################'''\n",
    "classifier.add(Convolution2D(64, kernel_size=(5, 5), strides=(1, 1), activation='relu'))\n",
    " \n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    " \n",
    "'''# STEP--3 FLattening'''\n",
    "classifier.add(Flatten())\n",
    " \n",
    "'''# STEP--4 Fully Connected Neural Network'''\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    " \n",
    "classifier.add(Dense(OutputNeurons, activation='softmax'))\n",
    " \n",
    "'''# Compiling the CNN'''\n",
    "#classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90c40b",
   "metadata": {},
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01f527ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.8658 - accuracy: 0.0779WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n",
      "8/8 [==============================] - 14s 1s/step - loss: 2.8658 - accuracy: 0.0779 - val_loss: 2.7610 - val_accuracy: 0.1066\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 5s 521ms/step - loss: 2.7537 - accuracy: 0.1107\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 4s 446ms/step - loss: 2.7029 - accuracy: 0.1516\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 3s 369ms/step - loss: 2.5639 - accuracy: 0.2336\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 3s 345ms/step - loss: 2.4236 - accuracy: 0.3320\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 3s 346ms/step - loss: 2.1706 - accuracy: 0.4385\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 3s 344ms/step - loss: 1.7998 - accuracy: 0.5123\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 3s 345ms/step - loss: 1.3792 - accuracy: 0.6516\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 3s 347ms/step - loss: 1.0239 - accuracy: 0.7377\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 3s 363ms/step - loss: 0.7497 - accuracy: 0.7910\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 3s 347ms/step - loss: 0.5483 - accuracy: 0.8811\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 3s 341ms/step - loss: 0.3790 - accuracy: 0.9057\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 3s 364ms/step - loss: 0.2708 - accuracy: 0.9262\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 3s 385ms/step - loss: 0.2333 - accuracy: 0.9262\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 3s 365ms/step - loss: 0.1820 - accuracy: 0.9426\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 6s 620ms/step - loss: 0.1118 - accuracy: 0.9713\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 3s 367ms/step - loss: 0.1091 - accuracy: 0.9754\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 3s 344ms/step - loss: 0.0446 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 5s 600ms/step - loss: 0.0469 - accuracy: 0.9877\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 0.0533 - accuracy: 0.9877\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 3s 397ms/step - loss: 0.0491 - accuracy: 0.9877\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 5s 602ms/step - loss: 0.0437 - accuracy: 0.9918\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 5s 599ms/step - loss: 0.0337 - accuracy: 0.9959\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 3s 352ms/step - loss: 0.0325 - accuracy: 0.9959\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 3s 365ms/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 5s 626ms/step - loss: 0.0252 - accuracy: 0.9959\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 3s 339ms/step - loss: 0.0225 - accuracy: 0.9918\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 3s 365ms/step - loss: 0.0232 - accuracy: 0.9959\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 6s 642ms/step - loss: 0.0178 - accuracy: 0.9959\n",
      "###### Total Time Taken:  2 Minutes ######\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Measuring the time taken by the model to train\n",
    "StartTime = time.time()\n",
    "\n",
    "# Manually repeat the generator for a specified number of epochs\n",
    "steps_per_epoch = len(training_set)\n",
    "\n",
    "# Starting the model training\n",
    "classifier.fit(\n",
    "    training_set,\n",
    "    epochs=30,\n",
    "    validation_data=test_set,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=10\n",
    ")\n",
    "\n",
    "EndTime = time.time()\n",
    "print(\"###### Total Time Taken: \", round((EndTime - StartTime) / 60), 'Minutes ######')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d57736",
   "metadata": {},
   "source": [
    "## Testing the CNN classifier on unseen images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5007ca",
   "metadata": {},
   "source": [
    "Using any one of the images from the testing data folder, we can check if the model is able to recognize the face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50172179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "{0: 'face1', 1: 'face10', 2: 'face11', 3: 'face12', 4: 'face13', 5: 'face14', 6: 'face15', 7: 'face16', 8: 'face2', 9: 'face3', 10: 'face4', 11: 'face5', 12: 'face6', 13: 'face7', 14: 'face8', 15: 'face9'}\n",
      "########################################\n",
      "Prediction is:  face3\n"
     ]
    }
   ],
   "source": [
    "'''########### Making single predictions ###########'''\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    " \n",
    "ImagePath='D:/Coding Course/IVY Pro School/ML/Face Images/Final Testing Images/face3/3face3.jpg'\n",
    "test_image=image.load_img(ImagePath,target_size=(64, 64))\n",
    "test_image=image.img_to_array(test_image)\n",
    " \n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    " \n",
    "result=classifier.predict(test_image,verbose=0)\n",
    "print(result)\n",
    "print(ResultMap)\n",
    "\n",
    "#print(training_set.class_indices)\n",
    " \n",
    "print('####'*10)\n",
    "print('Prediction is: ',ResultMap[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3351543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740587b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
